Notes for report:
V0.1:
Key Settings:
	> 4 UIC in 4 corners of the map
	> No ACA
	> Fast movement speed(25 Units)
	> 8 Directional Movement + Stand still
	> Static middle spawn (X:22, Y: 15)
	> Ray Perception Implementation, 180 Degrees, 50 Rays, 30 Ray Length
	> Max reward: 4(4 UIC)
Notes:
	> Original test, utilised a lot of what was taught in Week 9
	> Once a lot of little bugs were fixed, the agent was learning
	> Gravitated to one side(Leftwards) in first no bugs test
	> On average will get 1 or 2 civillians
	> This is potentially as a result of the large distance between the left and right groups of civillian rewards
	> Once it acquires two rewards will just wander around until episode end

V0.2
Key Settings Changes from V0.1:
	> 8 UIC spread about, still in a square shape
	> Slowed Movement Speed(20 Units)
	> Variable Middle spawn (Between X(17,27) and Y(10,20)
	> Max potential reward : 8(8 UIC)
Notes:
	> Similar start to v0.1, early gravitatiation to one direction(Left again)
	> Eventually though, consistently caught all 8 within 10 minutes of training and 200 steps
	> More civillians allowed the agent to chain together and reduce the weakness of longer range sensor data.
	> Some variation between full clears was observed, hinting that sensor data came into play to allow flexible adjustment, instead of a fixed path.
	> Opted to reimplement more functionality than settle
	
V0.3
Key Settings Changes from V0.2:
	> Addition of ACA(No movement) 2 Added closer than UIC
	> Code adjustments and sensor to account for ACA Addition
	> Further Speed decrease (15 Units)
	> Max Steps per episode doubled 2000->4000(In testing, it's more 200->400, i'm not sure but it's double time)
Notes:
	> Similar start to v0.2, Interestingly for a bit of time, the ML Agent would camp in the corner being scared of the ACA
	> Eventually it learned to do a path similar to v0.2 while avoiding the ACAs
	> It wouldn't aggressiely avoid ACAs but that's okay, they only gave a -0.25f rewards and would despawn
	> Other than that, not much changed between behaviour since low punishment ACAs aren't particularly detremental to the MLAgent
	> As a result of similar learning, it followed a similar learning policy akin to V0.2, but slower, took roughly 20000 more steps to reach max reward.

V0.4
Key Settings Changes from V0.3:
	> Basic Movement/Obstacle avoidance Implemented for UIC to add random movement
	> The units move a random x and y velocity between -1 and 1
	> They have basic Obstacle Avoidance(A1 Implementation that uses raycasts in 8 directions to push back)
	> No Flocking currently implemented
	> UIC Colliders changed from triggers to normal(They collide with each other and can throttle velocity/ Hit the wall with enough force)
	> More Units added, 2 more UIC(10 UIC Total), 2 more ACA(4 ACA Total)
	> ACA remains static. It's collider is kept as a trigger(Bug fixing resulted in them getting bounced around when I intended to keep them still for this version)
Notes:
	> Interestingly, with the added rng element of random movement/bumping on the UIC on episode start, the agent not only was able to learn, but learned quicker than v0.3
	> This is probably due to the sensors being a bit more powerful and the units trending towards middle and closer to each other whenever they coincidentally bump
	> Mean Scores of 7-9 were recorded as early as 6 minutes into training.
	> The MLAgent might not be respecting the ACAs as much as it should though, still barreling through them when in the real game, ACA capture is gameover. Although for testing this is completely okay, and great results.
	> Although I ended the training earlier than v0.3 I felt that only minor improvements will occur with the nature of rng, possibly a mistake, not sure.

V0.5
Key Settings Changes from V0.4:
	> First Iteration of readding Obstacles, only added buildings with original spots from A1
	> Minor movement of ACA to accomodate for Building, Still static
	> Covid Spreader can see obstacles at a range of 5, 16 Raycast Sensors, 180 degree view. No Punishments or Rewards applied. (Not sure how to handle this tbh)
	> Tightened up Spawn position so the MLAgent doesn't spawn in a building.
Notes:
	> Surprisingly, worked well, not sure if it's a natural part of ML or more a consequence of too few in-map obstacles, or the MLAgent utilising the Collidables sensor
	> Only learned a little slower than its v0.4 no obstacles cousin although it was a bit more random between rounds.
	> Still not respecting ACAs

v0.6
Key Settings Changes from V0.5:
	> Addition of props alongside buildings with efforts to integrate obstacles.
	> Significant Adjustment of both UIC and ACA spawns to accomodate for changes
	> ACA Is still static but now is slightly more punishing to the MLAgent for getting caught (From -0.25 to -0.5)
Notes:
	> Unsurprisingly, training took longer than v0.5
	> This is probably as a result of the fact that the MLAgent just can't run straight through obstacles and thus caught less on average per episode
	> Despite this, and despite the higher punishments, the training was somewhat successful.
	> ACA is slightly more respected
	> Potentially too many obstacles, MLAGent got stuck sometimes. Might iterate more on v0.6
	
v0.6.1
Key Settings Changes from v0.6:
	> Focus on iterating on current features.
	> Removal of some props, Movement of others
	> Max Steps increased to 5000
Notes:
	> I think the Changes were very well placed, extra steps allowed the MLAgent to learn faster with more time to navigate the space than v0.6
	> MLAgent got less stuck while still keeping a detailed playspace that looks and makes sense
	> MLAgent while it respects ACAs more, can still get quite scared in a corner near the end of an episode if no UICs are nearby
	> The MLAgent interestingly was a lot more consistent than previous versions within less steps despite more steps per episode
	> Nevertheless, still quite volatile per episode, this is probably due to the rng episodic nature.
	> Despite all this, the experiment was much stronger and we can now add more mechanics since the ML-Agent is still quite successful with the current featureset.
	> Will Consider longer training sessions 100-150k+ from now on to really nail down newer features.
	> Still Debating if a return to A1 Size or A1 Flocking/Fleeing is neccessary, the UIC running around randomly is kinda endearing :).
	
v0.7
Key Settings Changes from V0.6.1:
	> Removed two ACAs(From 4 to 2), Increased punishment (-0.5 to -0.75)
	> ACAs now use Arlo's AStar Pathfinding and try to catch the MLAgent, Their current speed is 5
	> Implemented neccesary changes to Arlo's Pathfinding(Cost Maps) to have it work on a smaller/different map
	> Max Steps increased to 6000
Notes:
	> This Training ran for 200k steps, significantly more than in the past
	> I think it consistently got maxish reward early akin to the previous version
	> Very satisfied with the results, despite the slightly higher negative potential punishment
	> Higher steps was pretty handy, once again but did feel a bit unneccessary towards the end
	> MLAgent learned very unique behaviour in dealing with the now mobile hostile ACAs
	> Sometimes correctly manipulated their pathfinding behaviour, even in some cases utilizing flaws in the pathfinding that even I as a human didn't figure out until observing the MLAgent play
	> Wasn't extremely consistent at avoidance, but I'd be lying if I didn't say they were doing a pretty good job towards the end
	> Despite the smaller map, the weaknesses of raysensor is very clear when it comes to max range detection though the low prio of far UICs could be a factor instead.
	> Probably gonna be the last feature implementation, Size seems somewhat unneccessary, readding flocking/fleeing to UIC could cause more issues than them running around headlessly, and it's kinda endearing.
	> ACAs seeking is a bit broken as well and the pathfinding works well enough in most cases.
	> An existing thought going through testing, is what is the proper way to test what should be a player actually chasing the MLAgent, the MLagent feels like it's learning habits that work well for Arlo's pathfinding, but not for a ACA Player which could cause issue.
	> Benefit is now that ACAs don't have a fixed position, the Covid Spreader isn't learning deadzones, now it can stall in the middle :D
	
v0.8
Key Settings Changes from V0.7:
	> Increased amount of UIC from 10 to 12(Max reward 12), Increased ACA Punishment from -0.75 to 1 to compensate for new max reward threshold and increased defensive movement
	> Decreased ACA Speed from 5 to 4 to better simulate intended player speed
	> Removed collision between UICs and ACAs and themselves(They still collide with walls and the Covid Spreader)
	> Removed a couple props, barriers at (5, 15)ish. Slight adjustments to episode civ spawn
Notes:
	> Not really different from how v0.7 went tbh
	> Despite wide variations and a slight increase in max rewards, it only took a bit slower to get to semi-consistent results.
	> Unfortunately, I think the limit of sensor training and features wanting to be added has been reached and even a late night training session will only offer marginal improvements over this kind of testing.
	> TBH I kind of want to change the settings of the yaml but the current settings are pretty good for getting desired results.
v0.9
Key Settings Changes from v0.8:
	> Potential bugfix for hanging ACAs when the MLAgent hugs near walls (Mostly (0, 0))
	> Reincreased ACA speed from 4 to 5
	> Gave ACAs very light obstacle avoidance (RC Length 1, Collision Strength 0.5)
Notes:
	> Same sort of success, the ACAs were more effective, still a bit buggy but the MLAgent was less abusive with exploitation
	> Not much else to say, Probably going to do a big (1 Million Steps) Training session to get a refined brain. Any changes are minor and the MLAgent is learning effectively with the current settings
v1.0
Key Settings Changes from v0.9:
	> Complete rework of the way training is done, if not effective, will be used for actual gameplay.
	> Episodes last infinitely, end when the ACA catches the Covid Spreader MLAgent
	> Random spawning for all entities at the start of the game and during the game when Covid Spreader catches UICs
	> As a result Theoretical episode length and reward score is infinite.
	> On capture, complete wipe of all entities and restart the game and episode.
	> Outside of this training change, most settings for the entities themselves are roughly the same.
	> Functions moved around but should be effectively the same game, just different training method that doesn't end and contain respawning UICs.
Notes:
	> Excellent Test, Started off slow but progressed quickly, occasional runs were 60+ captures before being caught in a training session that lasted only 200k Steps
	> A bit volatile but thats to be expected, certain spawns and manipulation are both beneficial and detrimental
	> Will probably give this a much longer test (1Mil) but keep the yaml settings, very satisfied with the results
	> The MLAgent learned very different behaviour since reward was constantly spawning rather than hanging towards the end.
	> Still was cheeky in manipulating the ACA roaming around but otherwise great success.
v1.1
Key Settings Changes from v1.0:
	> Changed settings to be similar to v0.9, Instead of episode end on capture, episode ends on a fixed step, 10000
	> Instead of completely set to inactive, the mobs are instead respawned during testing.
	> The idea is that v1.1 is a more appropriate testing pen than v1.0 and also to get a comparative result between 1.0 training and 1.1 training
	
All Major Tests
	> 1 Mil Step Count
	> Batch Size increased to 4096