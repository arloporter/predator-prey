Notes for report:
V0.1:
Key Settings:
	> 4 UIC in 4 corners of the map
	> No ACA
	> Fast movement speed(25 Units)
	> 8 Directional Movement + Stand still
	> Static middle spawn (X:22, Y: 15)
	> Ray Perception Implementation, 180 Degrees, 50 Rays, 30 Ray Length
	> Max reward: 4(4 UIC)
Notes:
	> Original test, utilised a lot of what was taught in Week 9
	> Once a lot of little bugs were fixed, the agent was learning
	> Gravitated to one side(Leftwards) in first no bugs test
	> On average will get 1 or 2 civillians
	> This is potentially as a result of the large distance between the left and right groups of civillian rewards
	> Once it acquires two rewards will just wander around until episode end

V0.2
Key Settings Changes from V0.1:
	> 8 UIC spread about, still in a square shape
	> Slowed Movement Speed(20 Units)
	> Variable Middle spawn (Between X(17,27) and Y(10,20)
	> Max potential reward : 8(8 UIC)
Notes:
	> Similar start to v0.1, early gravitatiation to one direction(Left again)
	> Eventually though, consistently caught all 8 within 10 minutes of training and 200 steps
	> More civillians allowed the agent to chain together and reduce the weakness of longer range sensor data.
	> Some variation between full clears was observed, hinting that sensor data came into play to allow flexible adjustment, instead of a fixed path.
	> Opted to reimplement more functionality than settle
	
V0.3
Key Settings Changes from V0.2:
	> Addition of ACA(No movement) 2 Added closer than UIC
	> Code adjustments and sensor to account for ACA Addition
	> Further Speed decrease (15 Units)
	> Max Steps per episode doubled 2000->4000(In testing, it's more 200->400, i'm not sure but it's double time)
Notes:
	> Similar start to v0.2, Interestingly for a bit of time, the ML Agent would camp in the corner being scared of the ACA
	> Eventually it learned to do a path similar to v0.2 while avoiding the ACAs
	> It wouldn't aggressiely avoid ACAs but that's okay, they only gave a -0.25f rewards and would despawn
	> Other than that, not much changed between behaviour since low punishment ACAs aren't particularly detremental to the MLAgent
	> As a result of similar learning, it followed a similar learning policy akin to V0.2, but slower, took roughly 20000 more steps to reach max reward.

V0.4
Key Settings Changes from V0.3:
	> Basic Movement/Obstacle avoidance Implemented for UIC to add random movement
	> The units move a random x and y velocity between -1 and 1
	> They have basic Obstacle Avoidance(A1 Implementation that uses raycasts in 8 directions to push back)
	> No Flocking currently implemented
	> UIC Colliders changed from triggers to normal(They collide with each other and can throttle velocity/ Hit the wall with enough force)
	> More Units added, 2 more UIC(10 UIC Total), 2 more ACA(4 ACA Total)
	> ACA remains static. It's collider is kept as a trigger(Bug fixing resulted in them getting bounced around when I intended to keep them still for this version)
Notes:
	> Interestingly, with the added rng element of random movement/bumping on the UIC on episode start, the agent not only was able to learn, but learned quicker than v0.3
	> This is probably due to the sensors being a bit more powerful and the units trending towards middle and closer to each other whenever they coincidentally bump
	> Mean Scores of 7-9 were recorded as early as 6 minutes into training.
	> The MLAgent might not be respecting the ACAs as much as it should though, still barreling through them when in the real game, ACA capture is gameover. Although for testing this is completely okay, and great results.
	> Although I ended the training earlier than v0.3 I felt that only minor improvements will occur with the nature of rng, possibly a mistake, not sure.

V0.5
Key Settings Changes from V0.4:
	> First Iteration of readding Obstacles, only added buildings with original spots from A1
	> Minor movement of ACA to accomodate for Building, Still static
	> Covid Spreader can see obstacles at a range of 5, 16 Raycast Sensors, 180 degree view. No Punishments or Rewards applied. (Not sure how to handle this tbh)
	> Tightened up Spawn position so the MLAgent doesn't spawn in a building.
Notes:
	> Surprisingly, worked well, not sure if it's a natural part of ML or more a consequence of too few in-map obstacles, or the MLAgent utilising the Collidables sensor
	> Only learned a little slower than its v0.4 no obstacles cousin although it was a bit more random between rounds.
	> Still not respecting ACAs

v0.6
Key Settings Changes from V0.5:
	> Addition of props alongside buildings with efforts to integrate obstacles.
	> Significant Adjustment of both UIC and ACA spawns to accomodate for changes
	> ACA Is still static but now is slightly more punishing to the MLAgent for getting caught (From -0.25 to -0.5)
Notes:
	> Unsurprisingly, training took longer than v0.5
	> This is probably as a result of the fact that the MLAgent just can't run straight through obstacles and thus caught less on average per episode
	> Despite this, and despite the higher punishments, the training was somewhat successful.
	> ACA is slightly more respected
	> Potentially too many obstacles, MLAGent got stuck sometimes. Might iterate more on v0.6
v0.6.1
Key Settings Changes from v0.6:
	> Focus on iterating on current features.
	> Removal of some props, Movement of others
	> Max Steps increased to 5000
Notes:
	> I think the Changes were very well placed, extra steps allowed the MLAgent to learn faster with more time to navigate the space
	> MLAgent got less stuck while still keeping a detailed playspace that looks and makes sense
	> MLAgent while it respects ACAs more, can still get quite scared in a corner near the end of an episode if no UICs are nearby
	